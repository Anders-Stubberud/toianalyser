{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fpdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m floor\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpolars\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfpdf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FPDF\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fpdf'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from math import floor\n",
    "import polars as pl\n",
    "from fpdf import FPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konstanter\n",
    "\n",
    "LIMIT_AXLES_SAME_GROUP = 1.8\n",
    "AXLE_WEIGHT = 'AxleWeight'\n",
    "AXLE_DISTANCE = 'AxleDistance'\n",
    "LIMIT_AXLE_WEIGHT = 30 # akselvekt over 30 tonn ansees som anomalier of filtreres vekk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_axle_load_distribution(filepath: str) -> tuple:\n",
    "\n",
    "    single_axle = np.array([])\n",
    "    boggi_axle = np.array([])\n",
    "    triple_axle = np.array([])\n",
    "\n",
    "    df = pl.read_csv(filepath, separator=';', truncate_ragged_lines=True, skip_rows=6, ignore_errors=True)\n",
    "    df = df.filter(pl.col('GrossWeight') > 3500)\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"StartTime\").cast(pl.Datetime(time_unit='ms')).alias('unix_timestamp')\n",
    "    )\n",
    "\n",
    "    earliest_date = df['unix_timestamp'].min()\n",
    "    latest_date = df['unix_timestamp'].max()\n",
    "\n",
    "    for row in df.iter_rows(named=True):\n",
    "        single, boggi, triple = calculate_axle_load_distribution_vehicle(row)\n",
    "        single_axle = np.concatenate((single_axle, single))\n",
    "        boggi_axle = np.concatenate((boggi_axle, boggi))\n",
    "        triple_axle = np.concatenate((triple_axle, triple))\n",
    "\n",
    "    return single_axle, boggi_axle, triple_axle, earliest_date, latest_date\n",
    "\n",
    "def calculate_axle_load_distribution_vehicle(row: tuple) -> tuple:\n",
    "    single_axle = np.array([])\n",
    "    boggi_axle = np.array([])\n",
    "    triple_axle = np.array([])\n",
    "\n",
    "    def distribute_weight(axles_in_group, weight_in_group):\n",
    "        nonlocal single_axle, boggi_axle, triple_axle\n",
    "        if axles_in_group == 1:\n",
    "            single_axle = np.append(single_axle, weight_in_group)\n",
    "        elif axles_in_group == 2:\n",
    "            boggi_axle = np.append(boggi_axle, weight_in_group)\n",
    "        elif axles_in_group == 3:\n",
    "            triple_axle = np.append(triple_axle, weight_in_group)\n",
    "\n",
    "    def axle_information(row: tuple, axle: int) -> tuple:\n",
    "        try:\n",
    "            has_axle_syntax_1 = f'{AXLE_DISTANCE}{axle}' in row and row[f'{AXLE_DISTANCE}{axle}'] != None\n",
    "            has_axle_syntax_2 = f'{AXLE_DISTANCE} {axle}' in row and row[f'{AXLE_DISTANCE} {axle}'] != None\n",
    "\n",
    "            if has_axle_syntax_1:\n",
    "                return True, float(row[f'{AXLE_DISTANCE}{axle}']), float(row[f'{AXLE_WEIGHT}{axle}']) / 1000\n",
    "            elif has_axle_syntax_2:\n",
    "                return True, float(row[f'{AXLE_DISTANCE} {axle}']), float(row[f'{AXLE_WEIGHT} {axle}']) / 1000\n",
    "        except:\n",
    "            # Støter på problemer her med at csv filene tolker data som datoer, får opp typ apr.33 etc,\n",
    "            # velger dermed å droppe kranglete akseldata\n",
    "            pass \n",
    "\n",
    "        return False, None, None\n",
    "\n",
    "    axle = 1\n",
    "    axles_in_group = 0\n",
    "    weight_in_group = 0\n",
    "\n",
    "    while True:\n",
    "        has_axle, distance_from_previous_axle, weight_axle = axle_information(row, axle)\n",
    "        if not has_axle:\n",
    "            break\n",
    "\n",
    "        if distance_from_previous_axle <= LIMIT_AXLES_SAME_GROUP:\n",
    "            axles_in_group += 1\n",
    "            weight_in_group += weight_axle\n",
    "\n",
    "        else:\n",
    "            distribute_weight(axles_in_group, weight_in_group)\n",
    "                \n",
    "            axles_in_group = 1\n",
    "            weight_in_group = weight_axle\n",
    "\n",
    "        axle += 1\n",
    "\n",
    "    distribute_weight(axles_in_group, weight_in_group)\n",
    "        \n",
    "    return single_axle, boggi_axle, triple_axle\n",
    "\n",
    "def extract_location(filepath):\n",
    "    if 'Aanestad' in filepath:\n",
    "        return 'Ånestad'\n",
    "    elif 'Øysand' in filepath:\n",
    "        return 'Øysand'\n",
    "    elif 'Skibotn' in filepath:\n",
    "        return 'Skibotn'\n",
    "    elif 'Verdal' in filepath:\n",
    "        return 'Verdal'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = (\n",
    "    '../WIM-data/Kistler_Øysand/20160808-31_Kistler Øysand_4913151-export(1).csv',\n",
    "    '../WIM-data/Kistler_Øysand/20160901-30_Kistler Øysand_4913151-export(2).csv',\n",
    "    '../WIM-data/Kistler_Øysand/20161001-31_Kistler Øysand_4913151-export(3)-fixed.csv',\n",
    "    '../WIM-data/Kistler_Øysand/20161101-30_Kistler Øysand_4913151-export(4).csv',\n",
    "    '../WIM-data/Kistler_Øysand/20161201-31_Kistler Øysand_4913151-export(5).csv',\n",
    "    '../WIM-data/Kistler_Øysand/20170101-31_Kistler Øysand_4913151-export(6).csv',\n",
    "    '../WIM-data/Kistler_Øysand/20170201-28_Kistler Øysand_4913151-export(7).csv',\n",
    "    '../WIM-data/Kistler_Øysand/20170301-31_Kistler Øysand_4913151-export(8).csv',\n",
    "    '../WIM-data/Kistler_Øysand/20170401-05_Kistler Øysand_4913151-export(9)-fixed.csv',\n",
    "    '../WIM-data/Kistler_Øysand/20180316_1.3.1_Kistler Øysand_4913151-export(24).csv',\n",
    "    '../WIM-data/Kistler_Øysand/20180401-30_Kistler Øysand_4796227-export(12).csv',\n",
    "    '../WIM-data/Kistler_Øysand/20180501-31(21-26)_Kistler Øysand_4796227-export(13).csv',\n",
    "    '../WIM-data/Kistler_Øysand/20180601-30(11-30)_Kistler Øysand_4796227-export(14).csv',\n",
    "    '../WIM-data/Kistler_Øysand/20180701-31(01-11)_Kistler Øysand_4796227-export(15).csv',\n",
    "    '../WIM-data/Kistler_Øysand/20180801-31(10-31)_Kistler Øysand_4796227-export(16).csv',\n",
    "    '../WIM-data/Kistler_Øysand/20180901-30_Kistler Øysand 4796227-export(17).csv',\n",
    "    '../WIM-data/Kistler_Skibotn/combinedFiles_E8_2018_kalibrert_4okt.csv',\n",
    "    '../WIM-data/Kistler_Skibotn/combinedFiles_E8_2019.csv',\n",
    "    '../WIM-data/Kistler_Skibotn/combinedFiles_E8_2020.csv',\n",
    "    '../WIM-data/Kistler_Verdal/20150513-20150531_Kistler Verdal 4796227.csv',\n",
    "    '../WIM-data/Kistler_Verdal/20150601-20150630_Kistler Verdal 4796227.csv',\n",
    "    '../WIM-data/Kistler_Verdal/20150701-20150731_Kistler Verdal 4796227.csv',\n",
    "    '../WIM-data/Kistler_Verdal/20150801-20150831_Kistler Verdal 4796227.csv',\n",
    "    '../WIM-data/Kistler_Verdal/20150901-20150930_Kistler Verdal 4796227.csv',\n",
    "    '../WIM-data/Kistler_Verdal/20151001-20151031_Kistler Verdal 4796227.csv',\n",
    "    '../WIM-data/Kistler_Verdal/20151101-20151130_Kistler Verdal 4796227.csv',\n",
    "    '../WIM-data/Kistler_Verdal/20151201-20151231_Kistler Verdal 4796227.csv',\n",
    "    '../WIM-data/Kistler_Verdal/20160101-20160131_Kistler Verdal 4796227.csv',\n",
    "    '../WIM-data/Kistler_Verdal/20160201-20160229_Kistler Verdal 4796227.csv',\n",
    "    '../WIM-data/Kistler_Verdal/20160301-20160331_Kistler Verdal 4796227.csv',\n",
    "    '../WIM-data/Kistler_Verdal/20160401-20160430_Kistler Verdal 4796227.csv',\n",
    "    '../WIM-data/Kistler_Verdal/20160501-20160531_Kistler Verdal 4796227.csv',\n",
    "    '../WIM-data/Kistler_Aanestad/20221014-20 Kistler_R3_ostg.csv',\n",
    "    '../WIM-data/Kistler_Aanestad/20221014-20 Kistler_R3_vestg.csv',\n",
    "    '../WIM-data/Kistler_Aanestad/20231001-20240123_Aanestad_Ostgående.csv',\n",
    "    '../WIM-data/Kistler_Aanestad/20231001-20240123_Aanestad_Vestgående.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = len(datasets)\n",
    "n_cols = 3\n",
    "data_information = ('Enkeltaksler', 'Boggiaksler', 'Trippelaksler')\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4.5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "ax_index = -1\n",
    "\n",
    "cols = ['location', 'axlegroup', 'startdate', 'enddate', 'mean', 'median'] + [f'{i}-{i+1} tonn' for i in range(30)]\n",
    "rows = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    try:\n",
    "        name_dataset = dataset.split('/')[-1]\n",
    "        location = extract_location(dataset)\n",
    "        *axle_load_distribution, earliest_date, latest_date = calculate_axle_load_distribution(dataset)\n",
    "        earliest_date = earliest_date.date().isoformat()\n",
    "        latest_date = latest_date.date().isoformat()\n",
    "\n",
    "        for i in range(0, 3):\n",
    "            data = axle_load_distribution[i]\n",
    "            mean = np.mean(data)\n",
    "            median = np.median(data)\n",
    "\n",
    "            bins = [0] * 30\n",
    "            for datapoint in data:\n",
    "                if datapoint >= LIMIT_AXLE_WEIGHT:\n",
    "                    continue\n",
    "                bins[floor(datapoint)] += 1\n",
    "            rows.append([location, data_information[i], earliest_date, latest_date, mean, median] + bins)\n",
    "\n",
    "            ax_index += 1\n",
    "            ax = axes[ax_index]\n",
    "            ax.hist(data, bins=30, color='blue', edgecolor='black')\n",
    "            ax.set_xlabel('Akselvekt (tonn)')\n",
    "            ax.set_ylabel('Passeringer')\n",
    "            ax.set_title(f'Dataset: {name_dataset}\\nAkselgruppe: {data_information[i]}', fontsize=10)\n",
    "            ax.text(0.5, -0.15, f'Snitt: {mean:.2f} | Median: {median:.2f}', ha='center', va='center', transform=ax.transAxes, fontsize=10)\n",
    "    \n",
    "    except:\n",
    "        print(dataset)\n",
    "\n",
    "df = pl.DataFrame(schema=cols, data=rows)\n",
    "df.write_csv('../resultater/aksellastfordeling.csv')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fpdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpolars\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfpdf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FPDF\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_pdf\u001b[39m(data, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      5\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m FPDF()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fpdf'"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from fpdf import FPDF\n",
    "\n",
    "def generate_pdf(data, filename='output.pdf'):\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "\n",
    "    # Set font\n",
    "    pdf.set_font('Arial', 'B', 12)\n",
    "\n",
    "    # Define column headers\n",
    "    headers = list(data[0].keys())\n",
    "    column_widths = [pdf.get_string_width(header) + 10 for header in headers]\n",
    "\n",
    "    # Add header row\n",
    "    for i, header in enumerate(headers):\n",
    "        pdf.cell(column_widths[i], 10, header, 1, 0, 'C')\n",
    "    pdf.ln()\n",
    "\n",
    "    # Add rows\n",
    "    pdf.set_font('Arial', '', 12)\n",
    "    for row in data:\n",
    "        for header in headers:\n",
    "            pdf.cell(column_widths[headers.index(header)], 10, str(row.get(header, '')), 1, 0, 'C')\n",
    "        pdf.ln()\n",
    "\n",
    "    # Save the PDF\n",
    "    pdf.output(filename)\n",
    "\n",
    "def pdf(dics):\n",
    "    generate_pdf(dics, 'dataframe_table.pdf')\n",
    "\n",
    "# Your main code\n",
    "df = pl.read_csv('../resultater/aksellastfordeling.csv')\n",
    "\n",
    "for axlegroup in ['Enkeltaksler', 'Boggiaksler', 'Trippelaksler']:\n",
    "    print(axlegroup)\n",
    "\n",
    "    weight_columns = [col for col in df.columns if '-' in col]\n",
    "    dfs = df.filter(pl.col('axlegroup') == axlegroup)\n",
    "    dfs = dfs.groupby('location').agg([\n",
    "        pl.col(col).sum().alias(col) for col in weight_columns\n",
    "    ])\n",
    "\n",
    "    column_sums = dfs.select([pl.col(c).sum().alias(c) for c in dfs.columns if (c != \"location\" and c != \"axlegroup\" and c != \"startdate\" and c != \"enddate\" and c != \"mean\" and c != \"median\")])\n",
    "\n",
    "    # Create a new DataFrame for the sum row with a name\n",
    "    sum_row_df = pl.DataFrame({\n",
    "        \"location\": [\"Total\"],\n",
    "        **column_sums.to_dict(False)\n",
    "    })\n",
    "\n",
    "    updated_df = dfs.vstack(sum_row_df)\n",
    "\n",
    "    dics = []\n",
    "\n",
    "    for row in updated_df.iter_rows(named=True):\n",
    "        sted = row['location']\n",
    "        sum_vekt = sum(row[vektspenn] for vektspenn in row.keys() if '-' in vektspenn)\n",
    "        dic = {'Sted': sted} | {vektspenn: 100 * row[vektspenn] / sum_vekt for vektspenn in row.keys() if '-' in vektspenn}\n",
    "        dics.append(dic)\n",
    "\n",
    "    pdf(dics, axlegroup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
